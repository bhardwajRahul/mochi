# Jun 2025 (v0.3.2)

Mochi v0.3.2 introduces **model aliases** and **embedding generation**, expanding the language’s integration with LLM-backed workflows. These additions make it easier to manage AI providers and extract semantic representations of text—supporting more advanced applications in ranking, similarity, and hybrid reasoning.

## Model Aliases

Mochi programs can now define reusable model configurations using `model` blocks. This allows prompts and generation logic to remain clean and declarative, while model settings are specified once and referenced by name.

```mochi
model fast {
  provider: "openai"
  name: "gpt-3.5-turbo"
}

let response = generate text {
  model: "fast"
  prompt: "Summarize this week’s release notes"
}
```

Model aliases apply to all forms of generation (`text`, `embedding`, `structured`) and delegate to the shared runtime backend.

## Embedding Generation

Mochi now supports generating vector embeddings from text using `generate embedding` blocks. The result is a `list<float>` suitable for downstream semantic tasks.

```mochi
let vec = generate embedding {
  text: "hello world"
  normalize: true
}

print(len(vec))
```

Normalization ensures the resulting vector has unit length, which is useful for cosine-based comparisons.

## Additional Updates

* All `generate` forms now route through a unified backend with simplified behavior
* Legacy interpolation logic inside `generate` blocks has been removed
* Internal test coverage has been expanded for embedding and alias support

